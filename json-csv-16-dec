import streamlit as st
import re
import json

def analyze_sql(sql_query):
    # Regular expression patterns for extracting source tables, columns, and filters
    table_pattern = re.compile(r'\bFROM\s+(\w+(\s+\w+)?)(\s*,\s*\w+(\s+\w+)?)?\b', re.IGNORECASE)
    column_pattern = re.compile(r'\bSELECT\s+([\w.,\s]+)\b', re.IGNORECASE)
    filter_pattern = re.compile(r'\bWHERE\s+(.+)\b', re.IGNORECASE)

    # Extract source tables, columns, and filters
    source_tables = table_pattern.findall(sql_query)
    source_columns = column_pattern.findall(sql_query)
    source_filters = filter_pattern.findall(sql_query)

    # Flatten lists and remove duplicates
    source_tables = list(set(item.strip() for sublist in source_tables for item in sublist if item))
    source_columns = list(set(item.strip() for sublist in source_columns for item in sublist.split(',')))
    source_filters = list(set(item.strip() for sublist in source_filters for item in sublist.split('AND')))

    # Create JSON output
    output_json = {
        "source_tables": source_tables,
        "source_columns": source_columns,
        "source_filters": source_filters
    }

    return json.dumps(output_json, indent=2)

def main():
    st.title("SQL Query Analyzer")

    # Get SQL query from user
    code_input = st.text_area("Enter SQL Query:")
    
    # Analyze button
    if st.button("Analyze SQL"):
        if code_input:
            # Analyze the SQL query
            result = analyze_sql(code_input)

            # Display the result
            st.header("Analysis Result (JSON):")
            st.json(result)
        else:
            st.warning("Please enter a SQL query.")

if __name__ == "__main__":
    main()
======================================================

import streamlit as st
import pandas as pd
from io import StringIO
from openai import AzureOpenAI
 
client= AzureOpenAI(
api_key = "44f6903de25643209d6bb1a98983f449",
api_version = "2023-09-15-preview",
azure_endpoint = "https://openai-pcs-az-cr10001-nab.openai.azure.com/"
 )


# Function to interact with chat and generate code snippets
def get_code_snippet(prompt):
    role1 = "You are a helpful Business Analyst.You help people to understand source to target mapping with filters.You also provide the output in structured spreadhseet format"

    response = client.chat.completions.create(
    model="gpt-35-turbo",
    messages=[
        {"role":"system", "content":role1},
        {"role":"user", "content": prompt}
    ],temperature=0.1
    )    
    return response.choices[0].message.content

# Streamlit application logic
st.title("SQL Query Analysis")

uploaded_file = st.file_uploader("Upload SQL file:", type=["txt"])

if uploaded_file:
    # Read the uploaded file
    sql_queries = [uploaded_file.read().decode("utf-8")]

    # Initialize empty dictionary for storing parsed data (key: query, value: extracted elements)
    parsed_data_dict = {}

    for query in sql_queries:
        # Prompt chat to parse the query and extract elements
        prompt = f"Parse the following SQL query and extract source tables, attributes/columns, and filters, ensuring no duplicates: {query}"
        output_data= get_code_snippet(prompt).strip()
        print(len(output_data),output_data)
        #for rec in output_data:
         #   print(rec)
          #  print("--------------------------------------------------------------")
        source_tables, attribute_column, filter,*data= output_data

        # Store extracted elements in a dictionary (key: query, value: set of elements)
        parsed_data_dict.setdefault(query, set()).update({
            "source_tables": source_tables, 
            "attribute_column": attribute_column, 
            "filter": filter,
        })

    # Combine extracted elements from all queries into a single dictionary
    combined_parsed_data = {}
    for query, elements in parsed_data_dict.items():
        combined_parsed_data.update({
            query: {key: list(value) for key, value in elements.items()}
        })

    # Convert parsed data to a list of dictionaries suitable for Streamlit table
    table_data = []
    for query, elements in combined_parsed_data.items():
        for source_table in elements["source_tables"]:
            for attribute_column in elements["attribute_column"]:
                for filter in elements["filter"]:
                    table_data.append({
                        "Query": query,
                        "Source_Table": source_table,
                        "Attribute/Column": attribute_column,
                        "Filter": filter,
                    })

    # Display parsed data in Streamlit table
    st.dataframe(table_data)

    # Download option (implement download_code_snippet function using chat)
    download_code_snippet = get_code_snippet("Generate Python code to download the parsed data as a spreadsheet with no duplicates")
    st.download_button("Download Spreadsheet", download_code_snippet, file_name="parsed_data.xlsx")

def download_code_snippet(parsed_data):
    # Prompt OpenAI chat to generate download code
    prompt = f"Based on the following parsed data (provide parsed_data dictionary), please generate Python code to download the information as a spreadsheet with columns for 'Query', 'Source_Table', 'Attribute/Column', and 'Filter'. Ensure duplicate entries are removed before generating the spreadsheet. You can choose any suitable library for creating and downloading the spreadsheet (e.g., openpyxl)."
    response = client.chat.completions.create(
    model="gpt-35-turbo",
    messages=[
    
        {"role":"user", "content": prompt}
    ],temperature=0.1
    )    
    return response.choices[0].message.content
    # Extract and return the generated code snippet
       
