import pandas as pd
from itertools import chain

# Read the CSV file into a Pandas DataFrame
csv_file_path = 'path/to/your/csv_file.csv'
df = pd.read_csv(csv_file_path)

# Split values in the 'Source Attribute' column into separate rows
df['Source Attribute'] = df['Source Attribute'].str.split(', ')
df_long = df.explode('Source Attribute')

# Drop the original 'Source Attribute' column
df_long.drop('Source Attribute', axis=1, inplace=True)

# Rename columns and remove leading/trailing whitespaces
df_long.columns = [col.strip() for col in df_long.columns]

# Display the modified DataFrame (optional)
print("Modified DataFrame:")
print(df_long)

# Save the modified DataFrame to a new CSV file
tabular_csv_file_path = 'path/to/output/tabular_format.csv'
df_long.to_csv(tabular_csv_file_path, index=False)

print(f"\nTabular format saved to: {tabular_csv_file_path}")
==================
google bard

import streamlit as st
import openai

# Set your OpenAI API key
OPENAI_API_KEY = "YOUR_OPENAI_KEY"

# Function to parse SQL using Codex
def parse_sql_with_codex(query):
    response = openai.CompletionRequest(
        model="code-davinci-002",
        prompt=f"Parse the following SQL query and extract source tables, target columns, and filters: {query}",
        temperature=0.7,
        log_probs=True,
    )
    completion = openai.OpenAI(OPENAI_API_KEY).complete(response)
    # Extract data from Codex response (implement extract_data_from_response() function)
    source_tables, target_columns, filters = extract_data_from_response(completion)
    return source_tables, target_columns, filters

# Function to generate spreadsheet download code using Codex
def generate_spreadsheet_download_code(parsed_data):
    prompt = f"Based on the following data (provide parsed_data), please write Python code to generate a downloadable spreadsheet with columns for source table, target column, and filter. Ensure duplicate entries are removed before generating the spreadsheet."
    response = openai.CompletionRequest(
        model="code-davinci-002",
        prompt=prompt,
        temperature=0.7,
        log_probs=True,
    )
    completion = openai.OpenAI(OPENAI_API_KEY).complete(response)
    # Extract and execute the generated code for downloading the spreadsheet
    download_code = completion.choices[0].text
    exec(download_code)

# Main application logic
def main():
    st.title("SQL Query Analysis")
    uploaded_file = st.file_uploader("Upload SQL file:", accept_types=["txt"])
    if uploaded_file:
        # Read the uploaded file
        sql_queries = uploaded_file.read().decode("utf-8").splitlines()
        # Analyze each query
        parsed_data = []
        for query in sql_queries:
            source_tables, target_columns, filters = parse_sql_with_codex(query)
            parsed_data.append({
                "source_tables": source_tables,
                "target_columns": target_columns,
                "filters": filters,
            })
        # Display parsed data in Streamlit table
        st.dataframe(parsed_data)

        # Generate and execute download code for spreadsheet
        download_code = generate_spreadsheet_download_code(parsed_data)
        download_button_label = "Download Spreadsheet"
        st.download_button(download_button_label, download_code, file_name="parsed_data.xlsx")

if __name__ == "__main__":
    main()
===================

def extract_data_from_response(completion):
    # Access the completion text
    text = completion.choices[0].text

    # Extract source tables
    source_tables_regex = r"Source tables: (.*)"
    match = re.search(source_tables_regex, text, re.IGNORECASE)
    source_tables = []
    if match:
        source_tables = [t.strip() for t in match.group(1).split(",")]

    # Extract target columns
    target_columns_regex = r"Target columns: (.*)"
    match = re.search(target_columns_regex, text, re.IGNORECASE)
    target_columns = []
    if match:
        target_columns = [t.strip() for t in match.group(1).split(",")]

    # Extract filters
    filters_regex = r"Filters: (.*)"
    match = re.search(filters_regex, text, re.IGNORECASE)
    filters = []
    if match:
        filters = [f.strip() for f in match.group(1).split(",")]

    # Remove duplicate entries
    source_tables = list(set(source_tables))
    target_columns = list(set(target_columns))
    filters = list(set(filters))

    return source_tables, target_columns, filters
===================================

import streamlit as st
from openai import Chat

# Initialize OpenAI chat
chat = Chat.from_api_key(YOUR_OPENAI_KEY)

# Function to interact with chat and generate code snippets
def get_code_snippet(prompt):
    response = chat.complete(prompt)
    # Extract and return the generated code snippet
    return response.choices[0].text

# Streamlit application logic
st.title("SQL Query Analysis")

uploaded_file = st.file_uploader("Upload SQL file:", accept_types=["txt"])

if uploaded_file:
    # Read the uploaded file
    sql_queries = uploaded_file.read().decode("utf-8").splitlines()

    # Initialize empty dictionary for storing parsed data (key: query, value: extracted elements)
    parsed_data_dict = {}

    for query in sql_queries:
        # Prompt chat to parse the query and extract elements
        prompt = f"Parse the following SQL query and extract source tables, attributes/columns, and filters, ensuring no duplicates: {query}"
        source_tables, attribute_column, filter = get_code_snippet(prompt).strip().split(", ")

        # Store extracted elements in a dictionary (key: query, value: set of elements)
        parsed_data_dict.setdefault(query, set()).update({
            "source_tables": source_tables, 
            "attribute_column": attribute_column, 
            "filter": filter,
        })

    # Combine extracted elements from all queries into a single dictionary
    combined_parsed_data = {}
    for query, elements in parsed_data_dict.items():
        combined_parsed_data.update({
            query: {key: list(value) for key, value in elements.items()}
        })

    # Convert parsed data to a list of dictionaries suitable for Streamlit table
    table_data = []
    for query, elements in combined_parsed_data.items():
        for source_table in elements["source_tables"]:
            for attribute_column in elements["attribute_column"]:
                for filter in elements["filter"]:
                    table_data.append({
                        "Query": query,
                        "Source_Table": source_table,
                        "Attribute/Column": attribute_column,
                        "Filter": filter,
                    })

    # Display parsed data in Streamlit table
    st.dataframe(table_data)

    # Download option (implement download_code_snippet function using chat)
    download_code_snippet = get_code_snippet("Generate Python code to download the parsed data as a spreadsheet with no duplicates")
    st.download_button("Download Spreadsheet", download_code_snippet, file_name="parsed_data.xlsx")
===
def download_code_snippet(parsed_data):
    # Prompt OpenAI chat to generate download code
    prompt = f"Based on the following parsed data (provide parsed_data dictionary), please generate Python code to download the information as a spreadsheet with columns for 'Query', 'Source_Table', 'Attribute/Column', and 'Filter'. Ensure duplicate entries are removed before generating the spreadsheet. You can choose any suitable library for creating and downloading the spreadsheet (e.g., openpyxl)."
    response = chat.complete(prompt)
    # Extract and return the generated code snippet
    return response.choices[0].text





