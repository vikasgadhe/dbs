import streamlit as st
import pandas as pd
import json
from io import StringIO
from openai.azure import AzureOpenAI

# Initialize OpenAI client
client = AzureOpenAI(
    api_key="**********************",
    api_version="2023-09-15-preview",
    azure_endpoint="https://openai-pcs-az-cr10001-nab.openai.azure.com/",
)

# Streamlit title and file upload
st.title("Data Lineage: Code Explainer")
uploaded_file = st.file_uploader("Upload a file:", type=["txt"])

# Process uploaded file (if any)
if uploaded_file is not None:
    # Read file content as string
    stringio = StringIO(uploaded_file.getvalue().decode("utf-8"))
    code_input = stringio.read()

    # Get uploaded file name for later use
    myscriptname = uploaded_file.name

    # Define function to explain code using OpenAI Codex
    def explain_code(input_code):
        role = "You are a helpful Business Analyst. You analyze SQL queries and extract each and every source table, source column with filter conditions if any used in the SQL in json. Please ensure all source tables, columns, and filters lists have the same length. Here is your json output format: [""Source Table:plants p"", ""Columns and Filters:t.type='Indoor'""]."

        # Format SQL prompt with emphasis on consistent lengths
        SQLprompt = f"Analyze the below SQL: \n\n {code_input} and extract each and every source table, source column and source filters applied. Ensure all source tables, columns, and filters lists have the same length. Create the output in json format. For Example: If the SQL text is ""Select p.plant_id as p_plant_id,f.plant_id as f_plant_id, NVL(p.color,'") as p_color from plants p, family f where p.plant_id = f.plant_id and f.family = 'Calathia'"" then the extact source tables will be plants p, family f and source columns and filters applied will be p.plant_id,f.plant_id,p.color,f.family ='Calathia'. Here is your json output format: [""Source Table:plants p"", ""Columns and Filters:t.type="Indoor"""],"

        # Send request to OpenAI and process response
        response = client.chat.completions.create(
            model="gpt-35-turbo",
            messages=[
                {"role": "system", "content": role},
                {"role": "user", "content": SQLprompt},
            ],
            temperature=0.8,
        )
        return response.choices[0].message.content

    # Analyze uploaded code and store results
    output_data = explain_code(code_input)

    # Download response as JSON file
    st.download_button("Download JSON", data=output_data, file_name=f"{myscriptname}.json")

    # Preprocess and convert JSON to DataFrame (optional)
    # You can modify this section based on your needs
    try:
        data = json.loads(output_data)
        # Check for consistent list lengths
        if len(data["Source Tables"]) != len(data["Columns and Filters"]):
            # Handle inconsistencies (e.g., raise error or pad lists)
            raise ValueError("Source tables and filters lists have different lengths.")

        # Convert to DataFrame and download as CSV (optional)
        df = pd.DataFrame(data)
        final = df.to_csv(f"{myscriptname}.csv", index=False)
        st.download_button("Download CSV", final, file_name=f"{myscriptname}.csv")
    except Exception as e:
        st.error(f"Error processing data: {e}")
========================

import streamlit as st
import pandas as pd
import json
from io import StringIO
from openai.azure import AzureOpenAI

# Initialize OpenAI client
client = AzureOpenAI(
    api_key="**********************",
    api_version="2023-09-15-preview",
    azure_endpoint="https://openai-pcs-az-cr10001-nab.openai.azure.com/",
)

# Streamlit title and file upload
st.title("Data Lineage: SQL Analyzer")
uploaded_file = st.file_uploader("Upload SQL file:", type=["txt"])

# Process uploaded file (if any)
if uploaded_file is not None:
    # Read file content as string
    stringio = StringIO(uploaded_file.getvalue().decode("utf-8"))
    code_input = stringio.read().strip()

    # Define function to analyze SQL query
    def analyze_sql(input_code):
        role = "You are a helpful Business Analyst. I analyze SQL queries and extract all source tables, columns, and filters applied. Ensure all lists have the same length. Here's the output format: [""Source Table: plants p"", ""Columns and Filters:t.type='Indoor'""]."
        SQLprompt = f"Analyze the below SQL: \n\n {input_code} and extract each source table, source column with filter conditions if any used in the SQL. Ensure all source tables, columns, and filters lists have the same length. Create the output in JSON format."

        # Send request to OpenAI and process response
        response = client.chat.completions.create(
            model="gpt-35-turbo",
            messages=[
                {"role": "system", "content": role},
                {"role": "user", "content": SQLprompt},
            ],
            temperature=0.8,
        )
        return json.loads(response.choices[0].message.content)

    # Analyze SQL and store results
    output_data = analyze_sql(code_input)

    # Download response as JSON file
    st.download_button("Download JSON", data=json.dumps(output_data, indent=4), file_name="sql_analysis.json")

    # Convert JSON to DataFrame and download as CSV (optional)
    # You can modify this section based on your needs
    df = pd.DataFrame(output_data)
    final = df.to_csv(f"sql_analysis.csv", index=False)
    st.download_button("Download CSV", final, file_name="sql_analysis.csv")

else:
    st.info("Please upload an SQL file for analysis.")


